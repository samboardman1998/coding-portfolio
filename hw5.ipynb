{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 4700 - Homework 5: Supervised Learning\n",
    "<h3 style=\"color: red;\">\n",
    "    Due: Wednesday, April 17, 1:24pm on CMS\n",
    "    <br><br>\n",
    "    Late submission: Friday, April 19, 1:24pm on CMS (50% penalty)\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which other students did you interact with on this assignment? Provide their NetID(s) below consistent with the [homework policy](http://www.cs.cornell.edu/courses/cs4700/2019sp/)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "WRITE NETID(S) HERE\n",
    "dr448"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework 5 will provide you with programming practice in the supervised learning environment. You will be implementing algorithms in Python 3, so please ensure your Jupyter Notebook environment is running the Python 3 kernel. Start by running the cell below. These are the **only** `import` statements allowed in this assignment. **Importing additional libraries will result in an automatic 0!**\n",
    "\n",
    "Throughout the assignment, you will find sections labeled as **<span style=\"color: red;\">Task</span>**. These sections mark the parts that you will need to work on. The other, un-marked sections simply provide information on the topics for reference.\n",
    "\n",
    "Lastly, most of your implementations for this homework will need to use functions from the `math` module. For those unfamiliar with the functions offered by this module, please refer to its documentation here: [https://docs.python.org/3/library/math.html](https://docs.python.org/3/library/math.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math    # copysign, exp, inf, log, pow\n",
    "import random  # shuffle, uniform\n",
    "import sys     # version_info\n",
    "\n",
    "assert sys.version_info >= (3,), 'You must use Python 3 for this assignment!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Logistic Regression\n",
    "\n",
    "<h3 style=\"text-align: right; margin-top: -1em;\">20 points</h3>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the **perceptron linear classifier** from lecture. The perceptron is a **single-layer neural network** that uses a hard threshold, which can cause issues for more complex data sets. One specific downside of this classifier is that it always outputs a confident result of 0 or 1; however, there are many cases where we would instead like to return a granular output, such as a probability. We can fix these issues by replacing the hard threshold function with a soft threshold function. One such function is the **logistic function**: $g \\left( z \\right) = \\frac{1}{1 + e^{-z}}$. The figure below shows a comparison between the two types of threshold functions.\n",
    "\n",
    "![](hard_vs_soft_threshold.png)\n",
    "\n",
    "Utilizing the logistic function as our threshold for a single-layer neural network results in the **logistic regression classifier**:\n",
    "\n",
    "$$\n",
    "h_\\overline{w} \\left( \\overline{x} \\right) = g \\left( \\overline{w} \\cdot \\overline{x} \\right) = \\frac{1}{1 + e^{-\\overline{w} \\cdot \\overline{x}}}\n",
    "$$\n",
    "\n",
    "In order for this model to learn the appropriate set of weights, we use the gradient descent method to minimize the error that our model makes. Given that the derivative of the logistic function is $g^\\prime \\left( z \\right) = g \\left( z \\right) \\times \\left( 1 - g \\left(z \\right) \\right)$, we can compute the gradient update for each weight (and for some learning rate $\\alpha$) as:\n",
    "\n",
    "$$\n",
    "w_i \\leftarrow w_i + \\alpha \\times \\left( y - g \\left( \\overline{w} \\cdot \\overline{x} \\right) \\right) \\times g^\\prime \\left(\\overline{w} \\cdot \\overline{x} \\right) \\times x_i\n",
    "$$\n",
    "\n",
    "Or equivalenty, in terms of only the logistic function, $g \\left( z \\right)$:\n",
    "\n",
    "$$\n",
    "w_i \\leftarrow w_i + \\alpha \\times \\left( y - g \\left( \\overline{w} \\cdot \\overline{x} \\right) \\right) \\times g \\left(\\overline{w} \\cdot \\overline{x} \\right) \\times \\left( 1 - g \\left( \\overline{w} \\cdot \\overline{x} \\right) \\right) \\times x_i\n",
    "$$\n",
    "\n",
    "Recall that the weights have an explicit \"bias\" term as $w_0$. This term has a corresponding entry in each example as a dummy value of 1 (i.e., $x_{i0} = 1$ for all entries $i$ in the data set). The figure below shows an abstract representation of our classifier.\n",
    "\n",
    "![](logistic_regression.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: red;\">Task:</h2>\n",
    "\n",
    "Implement the logistic regression classifier, using the pseudocode below as your guide. To help, you will need to implement two additional helper functions. `logistic` should implement the logistic function (refer to the previous formula). `dot_prod` should calculate the dot product of the two input vectors (leave the `assert` statement).\n",
    "\n",
    "---\n",
    "\n",
    "**Note, the pseudocode below is *slightly* different from what you saw in lecture. Here, we specify the stopping condition for the classifier as just iterating over the data set some number of times. This information is passed in as input to the classifier as `epochs`.**\n",
    "\n",
    "---\n",
    "\n",
    "$\n",
    "\\text{Inputs:} \\\\\n",
    "D - \\text{data set, a list of } \\left( \\overline{x},y \\right) \\text{, where } \\overline{x} \\in \\mathbb{R}^n+1 \\text{ and } y \\in \\{0,1\\} \\\\\n",
    "\\alpha - \\text{learning rate} \\\\\n",
    "\\text{epochs} - \\text{total number of iterations} \\\\\n",
    "\\ \\\\\n",
    "\\overline{\\underline{\\textbf{logistic_learn} \\left( D, \\alpha, \\text{epochs} \\right)}} \\\\\n",
    "\\quad \\overline{w} \\leftarrow \\overline{0} \\text{ of size } n+1 \\\\\n",
    "\\quad \\textbf{while} \\text{ epochs} > 0 \\\\\n",
    "\\qquad \\textbf{for each} \\left( \\overline{x},y \\right) \\in D \\\\\n",
    "\\qquad \\quad p \\leftarrow g \\left( \\overline{w} \\cdot \\overline{x} \\right) \\\\\n",
    "\\qquad \\quad \\textbf{for each} \\text{ feature } i \\\\\n",
    "\\qquad \\qquad w_i \\leftarrow w_i + \\alpha \\times \\left( y - p \\right) \\times p \\times \\left( 1 - p \\right) \\times x_i \\\\\n",
    "\\qquad \\text{epochs} \\leftarrow \\text{epochs} - 1 \\\\\n",
    "\\quad \\textbf{return } \\overline{w}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Returns the value of the logistic function evaluated at z\n",
    "\"\"\"\n",
    "def logistic(z):\n",
    "    return 1/(1 + math.exp(-z))\n",
    "\n",
    "\"\"\"\n",
    "Returns the dot product of xs and ys\n",
    "\"\"\"\n",
    "def dot_prod(xs, ys):\n",
    "    assert len(xs) == len(ys), 'Vector inputs must be of same size!'\n",
    "    sum = 0\n",
    "    for i in range(len(xs)):\n",
    "        sum += xs[i] * ys[i]\n",
    "    return sum\n",
    "\"\"\"\n",
    "Returns logistic regression coefficient vector by performing logistic regression on data data_set, with learning rate\n",
    "learning_rate, for epochs iterations\n",
    "\"\"\"\n",
    "def logistic_learn(data_set, learning_rate, epochs):\n",
    "    n = len(data_set[0][0]) - 1\n",
    "    w = list(0 for q in range(n+1))\n",
    "    while epochs > 0:\n",
    "        for (x, y) in data_set:\n",
    "            p = logistic(dot_prod(w,x))\n",
    "            for i in range(n+1):\n",
    "                w[i] = w[i] + learning_rate * (y - p) * p * (1-p) * x[i]\n",
    "        epochs = epochs - 1\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a sanity check, we have included some test cases that you can run your implementation against. The cell below will pass in a data set containing the 4 cases for the logical **AND** function. The logistic regression classifier should be able to learn the appropriate weights so that the 4 predictions are close to the actual labels. Try changing the learning rate and the number of epochs to get more accurate predictions. Your predictions should be within 0.1 of the true label for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 AND 0 is 0, you predicted 9.9819531064928e-07\n",
      "0 AND 1 is 0, you predicted 0.009353054511490051\n",
      "1 AND 0 is 0, you predicted 0.009354047995585923\n",
      "1 AND 1 is 1, you predicted 0.9889270199750348\n"
     ]
    }
   ],
   "source": [
    "and_set = [([1, 0, 0], 0), ([1, 0, 1], 0), ([1, 1, 0], 0), ([1, 1, 1], 1)]\n",
    "weights = logistic_learn(and_set, .99, 30000)\n",
    "for (x, y) in and_set:\n",
    "    pred = logistic(dot_prod(weights, x))\n",
    "    print('{} AND {} is {}, you predicted {}'.format(x[1], x[2], y, pred))\n",
    "    assert abs(pred - y) < 0.1, 'Your prediction diverged too much from the true label!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's ensure that your classifier is also able to learn the weights for the logical **OR** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 OR 0 is 0, you predicted 0.02331033960673716\n",
      "0 OR 1 is 1, you predicted 0.9853372900246228\n",
      "1 OR 0 is 1, you predicted 0.9853342752458599\n",
      "1 OR 1 is 1, you predicted 0.9999947138561152\n"
     ]
    }
   ],
   "source": [
    "or_set = [([1, 0, 0], 0), ([1, 0, 1], 1), ([1, 1, 0], 1), ([1, 1, 1], 1)]\n",
    "weights = logistic_learn(or_set, 0.5, 10000)\n",
    "for (x, y) in or_set:\n",
    "    pred = logistic(dot_prod(weights, x))\n",
    "    print('{} OR {} is {}, you predicted {}'.format(x[1], x[2], y, pred))\n",
    "    assert abs(pred - y) < 0.1, 'Your prediction diverged too much from the true label!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression classifier is still constrained in what functions it can represent. For example, it still fails to find weights for non-linearly separable data sets. The common example of such a data set is **XOR**. Run the cell below to verify for yourself that our logistic regression classifier is unable to produce correct weights that approximate the logical XOR function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 XOR 0 is 0, you predicted 0.51610600358629\n",
      "0 XOR 1 is 1, you predicted 0.5\n",
      "1 XOR 0 is 1, you predicted 0.4838939964137099\n",
      "1 XOR 1 is 0, you predicted 0.46782138179306076\n"
     ]
    }
   ],
   "source": [
    "xor_set = [([1, 0, 0], 0), ([1, 0, 1], 1), ([1, 1, 0], 1), ([1, 1, 1], 0)]\n",
    "weights = logistic_learn(xor_set, 0.5, 10000)\n",
    "for (x, y) in xor_set:\n",
    "    pred = logistic(dot_prod(weights, x))\n",
    "    print('{} XOR {} is {}, you predicted {}'.format(x[1], x[2], y, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Deep Neural Networks\n",
    "\n",
    "<h3 style=\"text-align: right; margin-top: -1em;\">40 points</h3>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To alleviate our issue of non-linearity, we need to replace our shallow, single-layer model. Both the perceptron and logistic regression classifiers have a single input layer and a single output node. But, to capture the non-linear relationship in data, we need additional layers! We refer to these, in general, as **hidden layers**. Combining this deep model of multiple layers with a soft threshold function (**activation function**) results in a **deep (multi-layered) neural network** (**DNN**). For simplicity, we will that each layer is fully connected to all nodes in the layer above it, allowing them to be represented as directed acyclic graphs of **units** or **neurons**. Another characteristic of DNNs is that they allow us to have multiple outputs, which is different from the classifiers we've explored so far. As a consequence, we now treat a data set $D$ to consist of $\\left(\\overline{x},\\overline{y}\\right)$, meaning both the data and label are vectors.\n",
    "\n",
    "The figure below shows an example of a DNN with 1 input layer (2 input units), 1 hidden layer (2 hidden units), and 1 output unit.\n",
    "\n",
    "![](deep_neural_network.png)\n",
    "\n",
    "When an input is passed into a DNN, the signals are passed along the network by following the connections between the units, starting at the input layer and ending in the output layer. This process is refered to as **forward propagation**. In order for the DNN to learn, it must update its weights across its multiple layers. The common approach for DNN learning is called **back-propagation**, which relies on the gradient descent method.\n",
    "\n",
    "The code cell below defines the `DNNUnit` class, which will represent a unit in a DNN. Creating an actual DNN is handled by the function `create_DNN`. Read through and understand the code below. **Of important note is the way the bias unit is handled: every unit in the network (except those in the input layer) has the bias unit as the first input/weight.** Run the cell to include these definitions to the environment.\n",
    "\n",
    "As an example, we can define the network from the figure above (assuming the logistic function is the activation function) as the following:\n",
    "```python\n",
    "logistic_deriv = lambda z : logistic(z) * (1 - logistic(z))\n",
    "figure_network = create_DNN(2, [2], 1, logistic, logistic_deriv)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A unit of a neural network.\n",
    "\n",
    "Inputs:\n",
    "  activation - the activation function to be used\n",
    "  deriv      - the derivative of the activation function\n",
    "  inputs     - a list of units that feed a signal to this unit\n",
    "  weights    - a list of weights that correspond to each input signal\n",
    "\"\"\"\n",
    "class DNNUnit:\n",
    "    def __init__(self, activation, deriv, inputs = None, weights = None):\n",
    "        self.activation = activation\n",
    "        self.deriv = deriv\n",
    "        self.inputs = inputs or []\n",
    "        self.weights = weights or []\n",
    "        self.value = None\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Creates a deep, multi-layered neural network. Includes a bias unit of value 1\n",
    "that is connected to every unit except those in the input layer. Initializes\n",
    "all weights to a random real number between -0.5 and 0.5, inclusive.\n",
    "\n",
    "Inputs:\n",
    "    input_layer_size   - the number of input units for the network\n",
    "    hidden_layer_sizes - a list with the number of hidden units in each hidden layer\n",
    "    output_layer_size  - the number of output units for the network\n",
    "    activation         - the activation function to be used\n",
    "    deriv              - the derivative of the activation function\n",
    "\"\"\"\n",
    "def create_DNN(input_layer_size, hidden_layer_sizes, output_layer_size, activation, deriv):\n",
    "    layer_sizes = [input_layer_size] + hidden_layer_sizes + [output_layer_size]\n",
    "    network = [[DNNUnit(activation, deriv) for _ in range(s)] for s in layer_sizes]\n",
    "    dummy_unit = DNNUnit(activation, deriv)\n",
    "    dummy_unit.value = 1.0\n",
    "    \n",
    "    # create weighted links\n",
    "    for k in range(1, len(layer_sizes)):\n",
    "        for unit in network[k]:\n",
    "            unit.inputs.append(dummy_unit)\n",
    "            unit.weights.append(random.uniform(-0.5, 0.5))\n",
    "            for input_unit in network[k - 1]:\n",
    "                unit.inputs.append(input_unit)\n",
    "                unit.weights.append(random.uniform(-0.5, 0.5))\n",
    "    \n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worthwhile to explore the interconnectedness between our above implementation of a DNN and its mathematical representation. The table below shows the attributes of a unit $i$ in a DNN, how to access it in code (let the variable be called `unit`), and its mathematical representation:\n",
    "\n",
    "| Description | Code | Math |\n",
    "| ----------- | ---- | ---- |\n",
    "| Activation function | `unit.activation` | $g\\left(z\\right)$ |\n",
    "| Derivative of activation function | `unit.deriv` | $g^\\prime\\left(z\\right)$ |\n",
    "| Units that connect to $i$ | `unit.inputs` | $\\overline{s}_i$ |\n",
    "| Weights for those units in $\\overline{s}_i$ | `unit.weights` | $\\overline{v}_i$ |\n",
    "| Value of the unit $i$ | `unit.value` | $a_i$ |\n",
    "\n",
    "Typically, when discussing about a unit $i$ in layer $k$, we also need to talk about the units that connect to $i$, the units that $i$ connects to, and their associated attributes. The table below summarizes the specific relationships we need for the propagation algorithms:\n",
    "\n",
    "| Description | Math |\n",
    "| ----------- | ---- |\n",
    "| Values for those units in $\\overline{s}_i$ | $\\overline{p}_i$ |\n",
    "| Units in layer $k+1$ that $i$ connects to | $\\overline{t}_i$ |\n",
    "| Weights between $i$ and those units in $\\overline{t}_i$ | $\\overline{u}_i$ |\n",
    "| Error for $i$ | $\\Delta_i$ |\n",
    "| Errors of those units in $\\overline{t}_i$ | $\\overline{q}_i$ |\n",
    "\n",
    "To make the above information slightly more concrete, let's look at a brief example. Consider the figure below. Note that unit 0 refers to the bias unit.\n",
    "\n",
    "![](dnn_example.png)\n",
    "\n",
    "For the figure above, we calculate the relevant information for unit 3 in the table below.\n",
    "\n",
    "| Math | Value |\n",
    "| ---- | ----- |\n",
    "| $\\overline{s}_3$ | $\\left<\\text{Unit 0, Unit 1, Unit 2}\\right>$ |\n",
    "| $\\overline{v}_3$ | $\\left<w_{0,3}\\text{, }w_{1,3}\\text{, }w_{2,3}\\right>$ |\n",
    "| $\\overline{p}_3$ | $\\left<a_0\\text{, }a_1\\text{, }a_2\\right>$ |\n",
    "| $\\overline{t}_3$ | $\\left<\\text{Unit 4, Unit 5, Unit 6}\\right>$ |\n",
    "| $\\overline{u}_3$ | $\\left<w_{3,4}\\text{, }w_{3,5}\\text{, }w_{3,6}\\right>$ |\n",
    "| $\\overline{q}_3$ | $\\left<\\Delta_4\\text{, }\\Delta_5\\text{, }\\Delta_6\\right>$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: red;\">Task:</h2>\n",
    "\n",
    "Implement the propagation algorithms, `forward_prop` and `back_prop`, using the pseudocode below as a guide.\n",
    "\n",
    "$\n",
    "\\text{Inputs:} \\\\\n",
    "\\text{DNN} - \\text{current state of the deep neural network,} \\\\\n",
    "\\quad \\text{where each layer } k \\text{ consists of units and each unit } i \\text{ has:} \\\\\n",
    "\\qquad \\bullet \\text{activation function } g \\\\\n",
    "\\qquad \\bullet \\text{derivative of activation function } g^\\prime \\\\\n",
    "\\qquad \\bullet \\text{vector of inputs } \\overline{s}_i \\\\\n",
    "\\qquad \\bullet \\text{vector of weights } \\overline{v}_i \\\\\n",
    "\\qquad \\bullet \\text{value } a_i \\\\\n",
    "\\overline{x} - \\text{current vector sample to be forward propagated} \\\\\n",
    "\\ \\\\\n",
    "\\overline{\\underline{\\textbf{forward_prop} \\left( \\text{DNN, } \\overline{x} \\right)}} \\\\\n",
    "\\quad \\textbf{for each} \\text{ unit } i \\in \\text{input layer} \\\\\n",
    "\\qquad a_i \\leftarrow x_i \\\\\n",
    "\\quad \\textbf{for each} \\text{ layer } k \\in \\text{DNN starting at second layer to output layer} \\\\\n",
    "\\qquad \\textbf{for each} \\text{ unit } i \\in k \\\\\n",
    "\\qquad \\quad \\overline{p}_i \\leftarrow \\text{vector of values for units in } \\overline{s}_i \\\\\n",
    "\\qquad \\quad a_i \\leftarrow g \\left( \\overline{v}_i \\cdot \\overline{p}_i \\right) \\\\\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "$\n",
    "\\text{Inputs:} \\\\\n",
    "\\text{DNN} - \\text{current state of the deep neural network,} \\\\\n",
    "\\quad \\text{where each layer } k \\text{ consists of units and each unit } i \\text{ has:} \\\\\n",
    "\\qquad \\bullet \\text{activation function } g \\\\\n",
    "\\qquad \\bullet \\text{derivative of activation function } g^\\prime \\\\\n",
    "\\qquad \\bullet \\text{vector of inputs } \\overline{s}_i \\\\\n",
    "\\qquad \\bullet \\text{vector of weights } \\overline{v}_i \\\\\n",
    "\\qquad \\bullet \\text{value } a_i \\\\\n",
    "D - \\text{data set, a list of} \\left( \\overline{x} , \\overline{y} \\right), \\text{where } \\overline{x} \\in \\mathbb{R}^{n} \\text{ and } \\overline{y} \\in \\mathbb{R}^{d} \\\\\n",
    "\\quad \\left( \\text{meaning } n \\text{ features and } d \\text{ number of outputs} \\right) \\\\\n",
    "\\alpha - \\text{learning rate} \\\\\n",
    "\\text{epochs} - \\text{total number of iterations} \\\\\n",
    "\\ \\\\\n",
    "\\overline{\\underline{\\textbf{back_prop} \\left( \\text{DNN,} D, \\alpha, \\text{epochs} \\right)}} \\\\\n",
    "\\quad \\Delta \\leftarrow \\text{vector of errors, initialized with units in DNN as keys and 0 for values} \\\\\n",
    "\\quad \\textbf{while } \\text{epochs} > 0 \\\\\n",
    "\\qquad \\textbf{for each} \\left( \\overline{x} , \\overline{y} \\right) \\in D \\\\\n",
    "\\qquad \\quad \\textbf{forward_prop} \\left( \\text{DNN, } \\overline{x} \\right) \\\\\n",
    "\\qquad \\quad \\textbf{for each} \\text{ unit } i \\in \\text{output layer} \\\\\n",
    "\\qquad \\qquad \\overline{p}_i \\leftarrow \\text{vector of values for units in } \\overline{s}_i \\\\\n",
    "\\qquad \\qquad \\Delta_i \\leftarrow g^\\prime \\left( \\overline{v}_i \\cdot \\overline{p}_i \\right) \\times \\left( y_i - a_i \\right) \\\\\n",
    "\\qquad \\quad \\textbf{for each} \\text{ layer } k \\text{ starting from last hidden layer to one above input layer} \\\\\n",
    "\\qquad \\qquad \\textbf{for each} \\text{ unit } i \\in k \\\\\n",
    "\\qquad \\qquad \\quad \\overline{p}_i \\leftarrow \\text{vector of values for units in } \\overline{s}_i \\\\\n",
    "\\qquad \\qquad \\quad \\overline{t}_i \\leftarrow \\text{vector of units in layer } k+1 \\text{ that } i \\text{ connects to} \\\\\n",
    "\\qquad \\qquad \\quad \\overline{u}_i \\leftarrow \\text{vector of weights between } i \\text{ and those units in } \\overline{t}_i \\\\\n",
    "\\qquad \\qquad \\quad \\overline{q}_i \\leftarrow \\text{vector of errors of those units in } \\overline{t}_i \\\\\n",
    "\\qquad \\qquad \\quad \\Delta_i \\leftarrow g^\\prime \\left( \\overline{v}_i \\cdot \\overline{p}_i \\right) \\times \\left( \\overline{u}_i \\cdot \\overline{q}_i \\right) \\\\\n",
    "\\qquad \\quad \\textbf{for each} \\text{ weight } w_{ij} \\text{ in DNN} \\\\\n",
    "\\qquad \\qquad w_{ij} \\leftarrow w_{ij} + \\alpha \\times a_i \\times \\Delta_j \\\\\n",
    "\\qquad \\text{epochs} \\leftarrow \\text{epochs} - 1\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Performs the forward propagation algorithm given above on DNN network and vector x\n",
    "\"\"\"\n",
    "def forward_prop(network, x):\n",
    "    count = -1\n",
    "    for i in network[0]:\n",
    "        count = count + 1\n",
    "        i.value = x[count]\n",
    "    for k in network[1:len(network)]:\n",
    "        for i in k:\n",
    "            p = list()\n",
    "            for j in range(0, len(i.inputs)):\n",
    "                p.append(i.inputs[j].value)\n",
    "            i.value = i.activation(dot_prod(i.weights, p))\n",
    "            \n",
    "\"\"\"\n",
    "Performs the backward propagation algorithm given above on DNN network, with data data_set and learning rate learning_rate, for\n",
    "epochs iterations\n",
    "\"\"\"\n",
    "def back_prop(network, data_set, learning_rate, epochs):\n",
    "    delta = {}\n",
    "    for k in range(0, len(network)):\n",
    "        for unit in network[k]:\n",
    "            delta[unit] = 0\n",
    "    while epochs > 0:\n",
    "        for (x, y) in data_set:\n",
    "            forward_prop(network, x)\n",
    "            count = -1\n",
    "            for i in network[len(network) - 1]:\n",
    "                count = count + 1\n",
    "                p = list()\n",
    "                for j in range(0, len(i.inputs)):\n",
    "                    p.append(i.inputs[j].value)\n",
    "                delta[i] = i.deriv(dot_prod(i.weights, p)) * (y[count] - i.value)\n",
    "            for k in reversed(range(1, len(network) - 1)):\n",
    "                count = -1\n",
    "                for i in network[k]:\n",
    "                    count = count + 1\n",
    "                    p = list()\n",
    "                    for j in range(0, len(i.inputs)):\n",
    "                        p.append(i.inputs[j].value)\n",
    "                    t = list()\n",
    "                    for j in range(1, len(network[k + 1])):\n",
    "                        t.append(network[k + 1][j])\n",
    "                    u = list()\n",
    "                    for g in t:\n",
    "                        u.append(g.weights[count + 1])\n",
    "                    q = list()\n",
    "                    for r in t:\n",
    "                        q.append(delta[r])\n",
    "                    delta[i] = i.deriv(dot_prod(i.weights, p)) * dot_prod(u, q)\n",
    "            for k in range(1, len(network)):\n",
    "                for j in network[k]:\n",
    "                    count = -1\n",
    "                    for i in j.inputs:\n",
    "                        count = count + 1\n",
    "                        j.weights[count] = j.weights[count] + learning_rate * i.value * delta[j]\n",
    "        epochs = epochs - 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: red;\">Task:</h2>\n",
    "\n",
    "If you have a working implementation of the algorithms, the DNN should be able to learn the **2-bit adder** function. For those unfamiliar, the following table summarizes this function's behavior:\n",
    "\n",
    "| $x_1$ | $x_2$ | $y_1$ | $y_2$ |\n",
    "| --- | --- | --- | --- |\n",
    "| 0 | 0 | 0 | 0 |\n",
    "| 0 | 1 | 0 | 1 |\n",
    "| 1 | 0 | 0 | 1 |\n",
    "| 1 | 1 | 1 | 0 |\n",
    "\n",
    "In short, the 2-bit adder has 2 outputs, where $y_1$ represents logical **AND** (linearly separable) and $y_2$ represents logical **XOR** (non-linearly separable). Run the code cell below multiple times to ensure your implementation is able to learn the 2-bit adder function. It is possible that the DNN will not approximate the function within an acceptable error-bound, even though your implementation is correct. **Why does this possibility exist?** Write your reasoning in the raw cell below."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ANSWER TO QUESTION HERE\n",
    "This is because, for binary classification with labels 0 and 1, the predicted value can be interpreted as the probability the true label is 1; there is inherent uncertainty in the true label, which is not necessarily small enough to meet an arbitrary accuracy threshold. Still, any prediction above .5 suggests 1 is the more likely label. Lastly, the initial weights are random, which affects their convergence trajectories via the backpropagation procedure to minimize the loss function locally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input = [0, 0]\n",
      "True Output = [0, 0]\n",
      "Prediction = [3.985743142435295e-06, 0.016219097655980366]\n",
      "\n",
      "Input = [0, 1]\n",
      "True Output = [0, 1]\n",
      "Prediction = [0.014475035462051937, 0.9782474439519478]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Your prediction diverged too much from the true output!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-740d80340503>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mdevs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mcorrect_out\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdevs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[1;32massert\u001b[0m \u001b[0md\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0.02\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Your prediction diverged too much from the true output!'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Your prediction diverged too much from the true output!"
     ]
    }
   ],
   "source": [
    "data_set = [([0,0],[0,0]), ([0,1],[0,1]), ([1,0],[0,1]), ([1,1],[1,0])]\n",
    "test_set = [[0,0], [0,1], [1,0], [1,1]]\n",
    "correct_out = [[0,0], [0,1], [0,1], [1,0]]\n",
    "logistic_deriv = lambda z : logistic(z) * (1 - logistic(z))\n",
    "network = create_DNN(2, [2,2], 2, logistic, logistic_deriv)\n",
    "\n",
    "back_prop(network, data_set, 0.5, 10000)\n",
    "for t in range(len(test_set)):\n",
    "    print('Input =', test_set[t])\n",
    "    print('True Output =', correct_out[t])\n",
    "    forward_prop(network, test_set[t])\n",
    "    preds = [node.value for node in network[-1]]\n",
    "    print('Prediction =', preds)\n",
    "    devs = [abs(preds[p] - correct_out[t][p]) for p in range(len(preds))]\n",
    "    for d in devs:\n",
    "        assert d <= 0.02, 'Your prediction diverged too much from the true output!'\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: red;\">Task:</h2>\n",
    "\n",
    "There are many types of activation functions that are used by DNNs. Implement the following set of activation functions and their derivatives. **Do not use the trigonometric functions from the `math` module in your implementation!** Their mathematical definitions are given as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\text{tanh} \\left( z \\right) &= \\frac{2}{1 + e^{-2z}} - 1 &\n",
    "    \\frac{d}{dz} \\text{tanh} &= 1 - \\text{tanh} \\left( z \\right)^2 \\\\\n",
    "    \\text{ReLU} \\left( z \\right) &=\n",
    "    \\begin{cases}\n",
    "        z & \\text{if } z > 0 \\\\\n",
    "        0 & \\text{otherwise}\n",
    "    \\end{cases} &\n",
    "    \\frac{d}{dz} \\text{ReLU} &= 0.5 + 0.5 \\times \\text{sgn} \\left( z \\right) \\\\\n",
    "    \\text{LeakyReLU} \\left( z \\right) &=\n",
    "    \\begin{cases}\n",
    "        z & \\text{if } z > 0 \\\\\n",
    "        0.01z & \\text{otherwise}\n",
    "    \\end{cases} &\n",
    "    \\frac{d}{dz} \\text{LeakyReLU} &= 0.505 + 0.495 \\times \\text{sgn} \\left( z \\right) \\\\\n",
    "    \\text{SmoothReLU} \\left( z \\right) &= \\log \\left( 1 + e^z \\right) &\n",
    "    \\frac{d}{dz} \\text{SmoothReLU} &= \\frac{1}{1 + e^{-z}} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "For those unfamiliar, $\\text{sgn}\\left(z\\right)$ refers to the sign function, which is defined as:\n",
    "\n",
    "$$\n",
    "\\text{sgn} \\left( z \\right) =\n",
    "\\begin{cases}\n",
    "    1 & \\text{if } z > 0 \\\\\n",
    "    0 & \\text{else if } z = 0 \\\\\n",
    "    -1 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "*Side note:* strictly speaking, the derivates for ReLU and LeakyReLU are undefined for 0. To avoid variations in how to handle this special case, we went ahead and defined the derivates as being the midpoint of the lower and upper values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Returns the value of the tanh function evaluated at z\n",
    "\"\"\"\n",
    "def tanh(z):\n",
    "    return 2/(1 + math.exp(-2 * z)) - 1\n",
    "\n",
    "\"\"\"\n",
    "Returns the derivative of the tanh function evaluated at z\n",
    "\"\"\"\n",
    "def tanh_deriv(z):\n",
    "    return 1 - math.pow(tanh(z), 2)\n",
    "    \n",
    "\"\"\"\n",
    "Returns the value of the ReLU function evaluated at z\n",
    "\"\"\"\n",
    "def relu(z):\n",
    "    if z > 0:\n",
    "        return z\n",
    "    return 0\n",
    "\n",
    "\"\"\"\n",
    "Returns the derivative of the ReLU function evaluated at z for z nonzero, .5 otherwise\n",
    "\"\"\"\n",
    "def relu_deriv(z):\n",
    "    if z > 0:\n",
    "        return 1\n",
    "    if z < 0:\n",
    "        return 0\n",
    "    return .5\n",
    "\n",
    "\"\"\"\n",
    "Returns the value of the LeakyReLU function evaluated at z\n",
    "\"\"\"\n",
    "def leaky_relu(z):\n",
    "    if z > 0:\n",
    "        return z\n",
    "    return .01 * z\n",
    "\n",
    "\"\"\"\n",
    "Returns the derivative of the ReLU function evaluated at z for z nonzero, .505 otherwise\n",
    "\"\"\"\n",
    "def leaky_relu_deriv(z):\n",
    "    if z > 0:\n",
    "        return 1\n",
    "    if z < 0:\n",
    "        return .01\n",
    "    return .505\n",
    "\n",
    "\"\"\"\n",
    "Returns the value of the SmoothReLU function evaluated at z\n",
    "\"\"\"\n",
    "def smooth_relu(z):\n",
    "    return math.log(1 + math.exp(z))\n",
    "\n",
    "\"\"\"\n",
    "Returns the derivative of the SmoothReLU function evaluated at z\n",
    "\"\"\"\n",
    "def smooth_relu_deriv(z):\n",
    "    return 1/(1 + math.exp(-z))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: red;\">Task:</h2>\n",
    "\n",
    "Now, let's compare the performance of the activation functions. To do this, we will use a different function for our DNN to approximate. Play around with the configurable variable values below and see how these **hyperparameters** (meaning parameters for the DNN hypothesis class) affect the performance for each activation function. **Which activation function(s) seem to work better for this problem?** Enter your response in the raw cell below."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ANSWER TO QUESTION HERE\n",
    "Increasing num_epochs substantially decreases the error rate initially (e.g. from 1000 to 10000) and does so asymptotically in the limit. Likewise, increasing learning_rate decreases the error rate. Of course, increasing the error bound usually decreases (and never increases) the error rate. The logistic function gives consistently low error rates (for num_epochs sufficiently large), whereas Leaky ReLu tends to oscillate between having the lowest error rate and having a moderate one; it only works well in certain random initial conditions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic error rate: 0.0 %\n",
      "Hypertangent error rate: 12.5 %\n",
      "ReLU error rate: 31.25 %\n",
      "Leaky ReLU error rate: 0.0 %\n",
      "Smooth ReLU error rate: 0.0 %\n"
     ]
    }
   ],
   "source": [
    "# configurable variables\n",
    "# play around with these values and see how it affects the error rate\n",
    "learning_rate = 0.5\n",
    "num_epochs = 5000\n",
    "error_bound = 0.2\n",
    "\n",
    "# code to test performance\n",
    "# do not modify\n",
    "data_set = [([p,q,r,s],[int(p and q), int(q or r), int(r <= s), int(s != p)]) for p in [0,1] for q in [0,1] for r in [0,1] for s in [0,1]]\n",
    "random.shuffle(data_set)\n",
    "training_set = data_set[0:12]\n",
    "logistic_deriv = lambda z : logistic(z) * (1 - logistic(z))\n",
    "func_names = [\"Logistic\", \"Hypertangent\", \"ReLU\", \"Leaky ReLU\", \"Smooth ReLU\"]\n",
    "act_funcs = [logistic, tanh, relu, leaky_relu, smooth_relu]\n",
    "deriv_funcs = [logistic_deriv, tanh_deriv, relu_deriv, leaky_relu_deriv, smooth_relu_deriv]\n",
    "r = random.getstate()\n",
    "for (n, a, d) in zip(func_names, act_funcs, deriv_funcs):\n",
    "    random.setstate(r)\n",
    "    network = create_DNN(4, [8,8], 4, a, d)\n",
    "    back_prop(network, data_set, learning_rate, num_epochs)\n",
    "    test_set = [x for (x,y) in data_set[12:]]\n",
    "    correct_out = [y for (x,y) in data_set[12:]]\n",
    "    num_misses = 0\n",
    "    for t in range(len(test_set)):\n",
    "        forward_prop(network, test_set[t])\n",
    "        preds = [unit.value for unit in network[-1]]\n",
    "        devs = [abs(preds[p] - correct_out[t][p]) for p in range(len(preds))]\n",
    "        for d in devs:\n",
    "            if d > error_bound:\n",
    "                num_misses += 1\n",
    "    print(n, 'error rate:', num_misses / 16.0 * 100.0, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Naïve Bayes\n",
    "\n",
    "<h3 style=\"text-align: right; margin-top: -1em;\">30 points</h3>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's turn our attention to the matter of probabilities. Part 1 mentioned how the logistic function has the nice feature of returning outputs that can be interpreted as probabilities. Let's continue with this train of thought. For this section, we will use $i$ to index over the $N$ samples in the data set, while $j$ will be used to index over the $n$ features in a particular sample $\\overline{x}_i$.\n",
    "\n",
    "In a supervised learning setting, we are interested in estimating $P\\left(Y \\mid X\\right)$, since we are given the data and want to predict the label. This is difficult to estimate, however, in a high dimensional setting. We could rely on Bayes' rule to help, but we end up with another issue when estimating $P\\left(X \\mid Y\\right)$. To make some progress, we take a huge leap of faith by making the **naïve Bayes assumption**, which states that the $n$ features in a sample are independent given the label: $P\\left(\\overline{x} \\mid y\\right) = \\prod_{j=1}^{n}P\\left(x_j \\mid y\\right)$.\n",
    "\n",
    "Taking all of this into account, we end up with the following **naïve Bayes classifier**:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "h\\left(\\overline{x}\\right) &= \\underset{y}{\\text{argmax }} P\\left(y \\mid \\overline{x}\\right) \\\\\n",
    "&= \\underset{y}{\\text{argmax }} \\frac{P\\left(\\overline{x} \\mid y\\right) P\\left(y\\right)}{P\\left(\\overline{x}\\right)} & \\text{Bayes' rule} \\\\\n",
    "&= \\underset{y}{\\text{argmax }} P\\left(\\overline{x} \\mid y\\right) P\\left(y\\right) & P\\left(\\overline{x}\\right) \\text{does not depend on } y \\\\\n",
    "&= \\underset{y}{\\text{argmax }} P\\left(y\\right) \\prod_{j=1}^{n} P\\left(x_j \\mid y\\right) & \\text{naïve Bayes assumption} \\\\\n",
    "&= \\underset{y}{\\text{argmax }} \\log\\left(P\\left(y\\right)\\right) + \\sum_{j=1}^{n} \\log\\left(P\\left(x_j \\mid y\\right)\\right) & \\log\\text{is monotonic}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Estimating $P\\left(y\\right)$ is usually straightforward. We can estimate the probability of the label $y$ having value $c$ with the following (where $I\\left(\\cdot\\right)$ is the indicator function, also defined below):\n",
    "\n",
    "$$\n",
    "P\\left(y=c\\right) = \\frac{1}{N} \\sum_{i=1}^{N} I\\left(y_i=c\\right) \\\\\n",
    "I\\left(x\\right) =\n",
    "\\begin{cases}\n",
    "    1 & \\text{if } x \\text{ is true} \\\\\n",
    "    0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "How about estimating $P\\left(x_j \\mid y\\right)$? This highly depends on what kind of features the data set contains. For this assignment, we will assume that we are dealing with **multinomial features**, meaning the features in the data represent counts. Thus, we have for any specific sample $\\overline{x}_i$:\n",
    "\n",
    "$$\n",
    "x_{ij} \\in \\{0,1,2,\\dots\\} \\text{ and } m_i = \\sum_{j=1}^{n} x_{ij}\n",
    "$$\n",
    "\n",
    "Note how $m_i$ can vary between samples, since not every sample's features will add up to the same sum. Under this assumption, we can precisely calculate the probability of a sample $\\overline{x}$ having the label value $c$ as:\n",
    "\n",
    "$$\n",
    "P\\left(\\overline{x}\\mid y=c\\right) = \\frac{\\left(\\sum_{j=1}^{n}x_j\\right)!}{\\prod_{j=1}^{n}x_j!} \\prod_{j=1}^{n} \\left(\\theta_{jc}\\right)^{x_j}\n",
    "$$\n",
    "\n",
    "We use $\\theta_{jc}$ as a parameter to refer to the probability of selecting feature $x_j$ and specify that $\\sum_{j=1}^{n}\\theta_{jc}=1$. While the above formula might seem intimidating, it does simplify when considered in the argmax setting of our classifier. This is because we can safely ignore the fraction as it does not depend on the label value $c$. We can estimate the $\\theta_{jc}$ parameter using the following equation (where $k$ is a smoothing parameter):\n",
    "\n",
    "$$\n",
    "\\theta_{jc} = \\frac{\\sum_{i=1}^{N} I\\left(y_i=c\\right)x_{ij}+k}{\\sum_{i=1}^{N} I\\left(y_i=c\\right)m_i+k\\times n}\n",
    "$$\n",
    "\n",
    "Finally, combining all of this together, we have that:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\underset{c}{\\text{argmax }}P\\left(y=c\\mid\\overline{x}\\right)\n",
    "&\\propto\\underset{c}{\\text{argmax }}P\\left(y=c\\right)\\prod_{j=1}^{n}\\left(\\theta_{jc}\\right)^{x_j} \\\\\n",
    "&\\propto\\underset{c}{\\text{argmax }}\\log\\left(P\\left(y=c\\right)\\right)+\\sum_{j=1}^{n}\\log\\left(\\left(\\theta_{jc}\\right)^{x_j}\\right) \\\\\n",
    "&\\propto\\underset{c}{\\text{argmax }}\\log\\left(P\\left(y=c\\right)\\right)+\\sum_{j=1}^{n}x_j\\log\\left(\\theta_{jc}\\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Therefore, for the naïve Bayes classifier (under a multinomial distribution) to make a prediction for a sample $\\overline{x}$, it would iterate over all label values to find the value $c$ that maximizes the above formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: red;\">Task:</h2>\n",
    "\n",
    "Implement the naïve Bayes classifier, `naive_bayes`, and its helper functions below. `label_est` should estimate $P\\left(y=c\\right)$. `theta_est` should estimate $\\theta_{jc}$. You may assume the label space $\\mathcal{Y}$ is binary, i.e., $y\\in\\{0,1\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Returns the estimated probability that a generic observation in data_set pertains to target_label\n",
    "\"\"\"\n",
    "def label_est(data_set, target_label):\n",
    "    sum = 0\n",
    "    for i in data_set:\n",
    "        if i[1] == target_label:\n",
    "            sum = sum + 1\n",
    "    return sum / len(data_set)\n",
    "\n",
    "\"\"\"\n",
    "Returns the paramter theta, using label target_label, attribute j, and smoothing parameter smooth, for the Naïve Bayes classifier\n",
    "created with data_set\n",
    "\"\"\"\n",
    "def theta_est(data_set, target_label, j, smooth):\n",
    "    num = 0\n",
    "    den = 0\n",
    "    for i in range(0, len(data_set)):\n",
    "        if data_set[i][1] == target_label:\n",
    "            num = num + data_set[i][0][j]\n",
    "            m_i = 0\n",
    "            for g in range(0, len(data_set[0][0])):\n",
    "                m_i = m_i + data_set[i][0][g]\n",
    "            den = den + m_i\n",
    "    num = num + smooth\n",
    "    den = den + smooth * len(data_set[0][0])\n",
    "    return(num / den)\n",
    "\n",
    "\"\"\"\n",
    "Returns the Naïve Bayes classifier (created with data_set) prediction of the label that most likely pertains to observation x,\n",
    "using smoothing parameter smooth\n",
    "\"\"\"\n",
    "def naive_bayes(data_set, x, smooth):\n",
    "    max = -math.inf\n",
    "    argmax = 0\n",
    "    for c in range(0, 2):\n",
    "        result = math.log(label_est(data_set, c))\n",
    "        for j in range(0, len(x)):\n",
    "            result = result + x[j] * math.log(theta_est(data_set, c, j, smooth))\n",
    "        if result > max:\n",
    "            max = result\n",
    "            argmax = c\n",
    "    return argmax\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: red;\">Task:</h2>\n",
    "\n",
    "Now let's test your implementation. The cell below has a pre-defined data set and test sample. At the moment, the smoothing parameter is set to 0 for the naïve Bayes classifier. Run the cell as-is and observe what the classifier predicts. Then, change the smoothing parameter to 1 and run the cell again. **What happens to the prediction? Why does this occur? What if we change the smoothing parameter to some integer greater than 1?** Give your answers in the raw cell below."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ANSWER TO QUESTIONS HERE\n",
    "The prediction changes from 0 to 1. This occurs because the addition of the smoothing parameter 1 creates k 'phantom' observations where y exhibits the target label, and attribute j is selected exactly once for each j. This modifies the values of the thetas, as shown above, which affects the calculation of the label that maximizes the probability of the observation assuming a label given the observation x. Changing the smoothing parameter to some integer greater than 1 retains the prediction of 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test sample: [9, 3]\n",
      "True label: 1\n",
      "Naive Bayes prediction (with smoothing 0): 0\n"
     ]
    }
   ],
   "source": [
    "data_set = [([4, 5], 1), ([4, 0], 0), ([3, 7], 1), ([0, 8], 1), ([2, 2], 0), ([1, 3], 0), ([6, 6], 1), ([9, 5], 1), ([1, 5], 1), ([3, 2], 1), ([5, 0], 1), ([4, 6], 1), ([0, 5], 1), ([0, 9], 1), ([2, 6], 1), ([1, 6], 1), ([4, 3], 1), ([6, 3], 1), ([8, 9], 1), ([2, 5], 1), ([1, 2], 0), ([3, 9], 1), ([5, 3], 1), ([3, 5], 1), ([8, 4], 1)]\n",
    "test_sample = ([9, 3], 1)\n",
    "smooth = 0\n",
    "pred = naive_bayes(data_set, test_sample[0], smooth)\n",
    "print('Test sample:', test_sample[0])\n",
    "print('True label:', test_sample[1])\n",
    "print('Naive Bayes prediction (with smoothing {}): {}'.format(smooth, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Just for Fun\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A set of points is said to be **shattered** by the set of all linear classifiers if, for all ways of assigning labels to the points, there exists a linear classifier that will give the points the assigned labels. Thus, for example, any set of three points in $\\mathbb{R}^2$ that are not colinear can be shattered by linear classifiers (see the figure below). However, all sets of four points (such as the four points from the XOR example) cannot be shattered. **What is the largest set of points that can be shattered for points in $\\mathbb{R}^3$**? Provide your answer in the raw text cell below.\n",
    "\n",
    "![](shattered_2d.png)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ANSWER TO QUESTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will only be submitting your Jupyter Notebook file, *hw5.ipynb*. Do not worry about submitting the additional files that came with the assignment. Furthermore, as a reminder, part of your grade is your documentation. Each of the functions you implemented as part of this assignment **must** be documented; failure to do so will result in a penalty.\n",
    "\n",
    "Please upload your *hw5.ipynb* file to CMS by **Wednesday, April 17 @ 1:24pm**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
